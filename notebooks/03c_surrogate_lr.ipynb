{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0276a00",
   "metadata": {},
   "source": [
    "# Credit Risk Assessment: Surrogate Logistic Regression\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e77ac804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aura.utils.pathing import models, reports, root\n",
    "import joblib\n",
    "import json\n",
    "import shap \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    precision_recall_curve, RocCurveDisplay, confusion_matrix,\n",
    "    classification_report, precision_score, recall_score, f1_score, PrecisionRecallDisplay\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import category_encoders as ce\n",
    "from scipy.stats import ks_2samp\n",
    "import scipy.stats as ss\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "stamp=date.today().isoformat()\n",
    "models = Path('../models')\n",
    "reports = Path('../reports')\n",
    "figs = Path('../reports/figs')\n",
    "processed = Path(\"../data/processed\")\n",
    "model_version = \"v1\"\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70043e8",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20746595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes (train/val/test): (1132562, 5) (181728, 5) (68061, 5)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"../data/processed/lc_cleaned.parquet\")\n",
    "cutoff_train = pd.to_datetime(\"2016-12-31\")\n",
    "cutoff_val = pd.to_datetime(\"2017-12-31\")\n",
    "\n",
    "train_idx = df[\"issue_d\"] <= cutoff_train\n",
    "val_idx = (df[\"issue_d\"] > cutoff_train) & (df[\"issue_d\"] <= cutoff_val)\n",
    "test_idx= df[\"issue_d\"] > cutoff_val\n",
    "\n",
    "X_train, y_train = df.loc[train_idx], df.loc[train_idx, \"default\"]\n",
    "X_val, y_val = df.loc[val_idx], df.loc[val_idx, \"default\"]\n",
    "X_test, y_test = df.loc[test_idx], df.loc[test_idx, \"default\"]\n",
    "\n",
    "ui_cols = [\"grade\", \"term\", \"acc_open_past_24mths\", \"dti\", \"fico_mid\"]\n",
    "\n",
    "X_train = X_train[ui_cols].copy()\n",
    "X_val = X_val[ui_cols].copy()\n",
    "X_test = X_test[ui_cols].copy()\n",
    "\n",
    "print(\"Shapes (train/val/test):\", X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dae4b53",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1c8dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ks_score(y, p): return ks_2samp(p[y==0], p[y==1]).statistic\n",
    "\n",
    "def profit_curve(y, p, gain_tp=.80, cost_fp=.10):\n",
    "    prec, rec, thr = precision_recall_curve(y, p)\n",
    "    tp = rec * y.sum(); fp = (tp / np.clip(prec,1e-9,1)) - tp\n",
    "    prof = (gain_tp*tp - cost_fp*fp) / len(y)\n",
    "    i = prof.argmax(); return float(thr[i]), float(prof[i])\n",
    "\n",
    "def best_f1(y, p):\n",
    "    prec, rec, thr = precision_recall_curve(y, p)\n",
    "    f1 = 2*prec*rec/(prec+rec+1e-12); return float(thr[f1.argmax()])\n",
    "\n",
    "def metrics(y, p, thr):\n",
    "    y_hat = (p > thr).astype(int)\n",
    "    return dict(AUC=roc_auc_score(y,p),\n",
    "                PR_AUC=average_precision_score(y,p),\n",
    "                KS=ks_score(y,p),\n",
    "                Precision=precision_score(y,y_hat),\n",
    "                Recall=recall_score(y,y_hat),\n",
    "                ConfMatrix=confusion_matrix(y,y_hat).tolist(),\n",
    "                ClassReport=classification_report(y,y_hat,\n",
    "                                                  digits=3, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cd4c62",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eb9552a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1382351, 187) (1132562, 5) (181728, 5) (68061, 5)\n"
     ]
    }
   ],
   "source": [
    "def engineer(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    z = df.copy()\n",
    "    z[\"grade_term\"] = z[\"grade\"].astype(str) + \"_\" + z[\"term\"].astype(str)\n",
    "    z[\"grade_term\"]= z[\"grade_term\"].astype(\"category\")\n",
    "    z[\"dti_inv\"] = 1.0 / (z[\"dti\"] + 1e-3)\n",
    "    z[\"fico_mid_sq\"] = z[\"fico_mid\"] ** 2\n",
    "    return z.drop(columns=[\"grade\",\"term\",\"dti\",\"fico_mid\"])\n",
    "\n",
    "X_train_fe = engineer(X_train)\n",
    "X_val_fe = engineer(X_val)\n",
    "X_test_fe = engineer(X_test)\n",
    "\n",
    "num_feats = [\"acc_open_past_24mths\", \"dti_inv\", \"fico_mid_sq\"]    \n",
    "cat_feats = [\"grade_term\"]\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    (\"imp\",   SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\", StandardScaler())\n",
    "])\n",
    "\n",
    "ohe_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\",\n",
    "                          sparse_output=True, min_frequency=0.005))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_feats),\n",
    "    (\"lowc\", ohe_pipe, cat_feats)\n",
    "], remainder=\"drop\")\n",
    "joblib.dump(preprocessor, models/f\"surrogate_lr_preprocessor_{model_version}.joblib\")\n",
    "\n",
    "print(df.shape, X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2241575c",
   "metadata": {},
   "source": [
    "### Save Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b03eaeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to /Users/pranavrao/Documents/ai-ml-projects/github-repos/aura-xai-finrisk-llm/data/processed\n"
     ]
    }
   ],
   "source": [
    "processed = Path(\"../data/processed\")\n",
    "X_train_fe.to_pickle(processed/\"X_train_sur.pkl\")\n",
    "X_val_fe.to_pickle(processed/\"X_val_sur.pkl\")\n",
    "X_test_fe.to_pickle(processed/\"X_test_sur.pkl\")\n",
    "pd.Series(y_train).to_csv(processed/\"y_train_sur.csv\", index=False)\n",
    "pd.Series(y_val).to_csv(processed/\"y_val_sur.csv\", index=False)\n",
    "pd.Series(y_test).to_csv(processed/\"y_test_sur.csv\", index=False)\n",
    "print(\"saved to\", processed.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579d1c32",
   "metadata": {},
   "source": [
    "### Surrogate Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76cef3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached surrogate\n"
     ]
    }
   ],
   "source": [
    "sur_path = models/f\"surrogate_lr_{model_version}.joblib\"\n",
    "meta_path= models / f\"surrogate_lr_meta_{model_version}.json\"\n",
    "\n",
    "if sur_path.exists():\n",
    "    print(\"Using cached surrogate\"); sur = joblib.load(sur_path)\n",
    "else:\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    base_lr = LogisticRegression(\n",
    "        penalty=\"elasticnet\", solver=\"saga\", max_iter=2000, tol=5e-4,\n",
    "        class_weight=\"balanced\", l1_ratio=0.5, n_jobs=1,\n",
    "        random_state=random_state, verbose=0\n",
    "    )\n",
    "    pipe = Pipeline([(\"pre\", preprocessor), (\"clf\", base_lr)],\n",
    "                    memory=joblib.Memory(\"./cache\"))\n",
    "    param_dist = {\n",
    "        \"clf__C\": ss.loguniform(5e-2, 5),\n",
    "        \"clf__l1_ratio\": ss.uniform(0, 1)\n",
    "    }\n",
    "\n",
    "    search = RandomizedSearchCV(pipe, param_dist,\n",
    "                                n_iter=20, scoring=\"roc_auc\",\n",
    "                                cv=tscv, n_jobs=3, random_state=random_state,\n",
    "                                verbose=2).fit(X_train_fe, y_train)\n",
    "    best_C = search.best_params_[\"clf__C\"]\n",
    "    best_l1 = search.best_params_[\"clf__l1_ratio\"]\n",
    "    print(\"Best AUC (CV):\", search.best_score_)\n",
    "\n",
    "    final_lr = LogisticRegression(\n",
    "        penalty=\"elasticnet\", solver=\"saga\", C=best_C, l1_ratio=best_l1,\n",
    "        max_iter=2000, tol=5e-4, class_weight=\"balanced\",\n",
    "        n_jobs=1, random_state=random_state, verbose=0\n",
    "    )\n",
    "    final_pipe = Pipeline([(\"pre\", preprocessor), (\"clf\", final_lr)])\n",
    "    final_pipe.fit(X_train_fe, y_train)\n",
    "\n",
    "    sur = CalibratedClassifierCV(final_pipe, method=\"isotonic\", cv=\"prefit\")\n",
    "    sur.fit(X_val_fe, y_val)\n",
    "\n",
    "    joblib.dump(sur, sur_path)\n",
    "    json.dump({\"date\": date.today().isoformat(),\n",
    "               \"UI_inputs\": ui_cols,\n",
    "               \"eng_feats\": num_feats + cat_feats,\n",
    "               \"best_C\": float(best_C),\n",
    "               \"best_l1\": float(best_l1)},\n",
    "              open(meta_path,\"w\"), indent=2)\n",
    "    print(\"Surrogate model + meta written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e60d67",
   "metadata": {},
   "source": [
    "### Evaluation Metrics - Train/Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7dbc776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN (F1-opt) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.874     0.659     0.751    901143\n",
      "           1      0.322     0.631     0.426    231419\n",
      "\n",
      "    accuracy                          0.653   1132562\n",
      "   macro avg      0.598     0.645     0.589   1132562\n",
      "weighted avg      0.761     0.653     0.685   1132562\n",
      "\n",
      "\n",
      "=== VALID (F1-opt) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.838     0.460     0.594    130152\n",
      "           1      0.363     0.775     0.494     51576\n",
      "\n",
      "    accuracy                          0.549    181728\n",
      "   macro avg      0.600     0.618     0.544    181728\n",
      "weighted avg      0.703     0.549     0.565    181728\n",
      "\n",
      "\n",
      "=== VALID (profit-opt) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.915     0.147     0.253    130152\n",
      "           1      0.310     0.965     0.469     51576\n",
      "\n",
      "    accuracy                          0.379    181728\n",
      "   macro avg      0.612     0.556     0.361    181728\n",
      "weighted avg      0.743     0.379     0.314    181728\n",
      "\n",
      "ROC & PR curves saved for train + val\n",
      "train & validation curves saved and markdown written to metrics_sur_trainval_v1.md\n"
     ]
    }
   ],
   "source": [
    "p_train = sur.predict_proba(X_train_fe)[:, 1]\n",
    "p_val = sur.predict_proba(X_val_fe)[:,1]\n",
    "\n",
    "thr_f1_train = best_f1(y_train, p_train)\n",
    "thr_f1_val = best_f1(y_val, p_val)\n",
    "thr_profit_val, _ = profit_curve(y_val, p_val)\n",
    "\n",
    "m_train = metrics(y_train, p_train, thr_f1_train)\n",
    "m_val_f = metrics(y_val, p_val, thr_f1_val)\n",
    "m_val_p = metrics(y_val,p_val,thr_profit_val)\n",
    "\n",
    "print(\"=== TRAIN (F1-opt) ===\")\n",
    "print(m_train[\"ClassReport\"])\n",
    "print(\"\\n=== VALID (F1-opt) ===\")\n",
    "print(m_val_f[\"ClassReport\"])\n",
    "print(\"\\n=== VALID (profit-opt) ===\")\n",
    "print(m_val_p[\"ClassReport\"])\n",
    "\n",
    "def save_curves(split:str, y_true, proba):\n",
    "    roc = RocCurveDisplay.from_predictions(y_true, proba, name=f\"LR – {split.upper()}\")\n",
    "    roc.figure_.savefig(figs / f\"roc_sur_{split}_{model_version}.png\", dpi=300)\n",
    "    plt.close(roc.figure_)\n",
    "\n",
    "    pr = PrecisionRecallDisplay.from_predictions(y_true, proba, name=f\"LR – {split.upper()}\")\n",
    "    pr.figure_.savefig(figs/f\"pr_sur_{split}_{model_version}.png\", dpi=300)\n",
    "    plt.close(pr.figure_)\n",
    "\n",
    "save_curves(\"train\", y_train, p_train)\n",
    "save_curves(\"val\",y_val,p_val)\n",
    "print(\"ROC & PR curves saved for train + val\")\n",
    "\n",
    "md_path = reports / f\"metrics_sur_trainval_{model_version}.md\"\n",
    "with open(md_path, \"w\") as f:\n",
    "    f.write(f\"# Logistic Regression – {model_version}  \\n\")\n",
    "    f.write(f\"*Date generated:* {date.today().isoformat()}\\n\\n\")\n",
    "\n",
    "    # ---------- thresholds ----------\n",
    "    f.write(\"## Thresholds chosen\\n\")\n",
    "    f.write(\"| Split | F1-opt | Profit-opt |\\n\")\n",
    "    f.write(\"|-------|-------:|-----------:|\\n\")\n",
    "    f.write(f\"| Train | {thr_f1_train:.3f} | – |\\n\")\n",
    "    f.write(f\"| Val | {thr_f1_val:.3f} | {thr_profit_val:.3f} |\\n\\n\")\n",
    "\n",
    "    # ---------- TRAIN ----------\n",
    "    f.write(\"## Train (F1-optimised)\\n\")\n",
    "    f.write(f\"- **AUC:** `{m_train['AUC']:.4f}`  \\n\")\n",
    "    f.write(f\"- **PR-AUC:** `{m_train['PR_AUC']:.4f}`  \\n\")\n",
    "    f.write(f\"- **KS:** `{m_train['KS']:.4f}`  \\n\\n\")\n",
    "    f.write(\"```text\\n\" + m_train[\"ClassReport\"] + \"\\n```\\n\\n\")\n",
    "\n",
    "    # ---------- VAL: F1 ----------\n",
    "    f.write(\"## Validation (F1-optimised)\\n\")\n",
    "    f.write(f\"- **AUC:** `{m_val_f['AUC']:.4f}`  \\n\")\n",
    "    f.write(f\"- **PR-AUC:** `{m_val_f['PR_AUC']:.4f}`  \\n\")\n",
    "    f.write(f\"- **KS:** `{m_val_f['KS']:.4f}`  \\n\\n\")\n",
    "    f.write(\"```text\\n\" + m_val_f[\"ClassReport\"] + \"\\n```\\n\\n\")\n",
    "\n",
    "    # ---------- VAL: Profit ----------\n",
    "    f.write(\"## Validation (Profit-optimised)\\n\")\n",
    "    f.write(f\"- **AUC:** `{m_val_p['AUC']:.4f}`  \\n\")\n",
    "    f.write(f\"- **PR-AUC:** `{m_val_p['PR_AUC']:.4f}`  \\n\")\n",
    "    f.write(f\"- **KS:** `{m_val_p['KS']:.4f}`  \\n\\n\")\n",
    "    f.write(\"```text\\n\" + m_val_p['ClassReport'] + \"\\n```\\n\")\n",
    "\n",
    "print(\"train & validation curves saved and markdown written to\", md_path.relative_to(reports))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dac79c5",
   "metadata": {},
   "source": [
    "### Evaluation Metrics - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd28ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST (F1-opt) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.815     0.463     0.590     47444\n",
      "           1      0.380     0.759     0.507     20617\n",
      "\n",
      "    accuracy                          0.553     68061\n",
      "   macro avg      0.598     0.611     0.549     68061\n",
      "weighted avg      0.684     0.553     0.565     68061\n",
      "\n",
      "\n",
      "=== TEST (profit-opt) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.895     0.124     0.218     47444\n",
      "           1      0.324     0.966     0.485     20617\n",
      "\n",
      "    accuracy                          0.379     68061\n",
      "   macro avg      0.609     0.545     0.351     68061\n",
      "weighted avg      0.722     0.379     0.299     68061\n",
      "\n",
      "test curves saved and markdown updated to metrics_sur_test_v1.md\n"
     ]
    }
   ],
   "source": [
    "p_test = sur.predict_proba(X_test_fe)[:, 1]\n",
    "thr_f1_test = best_f1(y_test, p_test)\n",
    "thr_profit_test, _ = profit_curve(y_test, p_test)\n",
    "\n",
    "m_test_f = metrics(y_test, p_test, thr_f1_test)\n",
    "m_test_p = metrics(y_test, p_test, thr_profit_test)\n",
    "\n",
    "print(\"=== TEST (F1-opt) ===\")\n",
    "print(m_test_f[\"ClassReport\"])\n",
    "print(\"\\n=== TEST (profit-opt) ===\")\n",
    "print(m_test_p[\"ClassReport\"])\n",
    "\n",
    "roc = RocCurveDisplay.from_predictions(y_test, p_test, name=\"LR – TEST\")\n",
    "roc.figure_.savefig(figs / f\"roc_sur_test_{model_version}.png\", dpi=300)\n",
    "plt.close(roc.figure_)\n",
    "\n",
    "pr  = PrecisionRecallDisplay.from_predictions(y_test, p_test, name=\"LR – TEST\")\n",
    "pr.figure_.savefig(figs / f\"pr_sur_test_{model_version}.png\", dpi=300)\n",
    "plt.close(pr.figure_)\n",
    "\n",
    "md_path = reports / f\"metrics_sur_test_{model_version}.md\"\n",
    "with open(md_path, \"a\") as f:          \n",
    "    f.write(\"\\n---\\n\\n\")\n",
    "    f.write(\"## Test results\\n\")\n",
    "    f.write(\"| Threshold type | Value |\\n\")\n",
    "    f.write(\"|----------------|------:|\\n\")\n",
    "    f.write(f\"| F1-opt | {thr_f1_test:.3f} |\\n\")\n",
    "    f.write(f\"| Profit-opt | {thr_profit_test:.3f} |\\n\\n\")\n",
    "\n",
    "    # F1-optimised block\n",
    "    f.write(\"### Test (F1-optimised)\\n\")\n",
    "    f.write(f\"- **AUC:** `{m_test_f['AUC']:.4f}`  \\n\")\n",
    "    f.write(f\"- **PR-AUC:** `{m_test_f['PR_AUC']:.4f}`  \\n\")\n",
    "    f.write(f\"- **KS:** `{m_test_f['KS']:.4f}`  \\n\\n\")\n",
    "    f.write(\"```text\\n\" + m_test_f[\"ClassReport\"] + \"\\n```\\n\\n\")\n",
    "\n",
    "    # Profit-optimised block\n",
    "    f.write(\"### Test (Profit-optimised)\\n\")\n",
    "    f.write(f\"- **AUC:** `{m_test_p['AUC']:.4f}`  \\n\")\n",
    "    f.write(f\"- **PR-AUC:** `{m_test_p['PR_AUC']:.4f}`  \\n\")\n",
    "    f.write(f\"- **KS:** `{m_test_p['KS']:.4f}`  \\n\\n\")\n",
    "    f.write(\"```text\\n\" + m_test_p['ClassReport'] + \"\\n```\\n\")\n",
    "\n",
    "print(\"test curves saved and markdown updated to\", md_path.relative_to(reports))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03441a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
